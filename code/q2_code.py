# -*- coding: utf-8 -*-
"""Q2 Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y4dc11vh-9QrpB553c04gI9T90eKr6gy
"""

#Experimentation:
import pandas as pd

trainFilename = input("Specify your csv formatted training data ('train.csv'): ")
testFilename  = input("Specify your csv formatted testing data ('test.csv'): ")
algorithm = input("Determine the superparent using 1) Mutual Information or 2) Cramer'sV with the Class (1/2): ")
k = int(input("Specify the value of the smoothing parameter (#): "))
m = int(input("Number of AISWNB generations (#): "))

training = pd.read_csv(trainFilename)
testing = pd.read_csv(testFilename)

parent, attrVals, PT, lProbs, plProbs = spTan(training, algorithm, k)
weights = AISWNB(training, parent, PT, lProbs, plProbs, m=m)
test(testing, parent, k, attrVals, PT, lProbs, plProbs, weights)

#SuperParent Tan Algorithm:
def spTan(dataset, algorithm, k):
  attributes = dataset.columns.tolist()
  className = attributes.pop()
  size = len(dataset)
  labels = dataset[className].unique()
  attrVals = {attr:dataset[attr].unique() for attr in attributes}

  if algorithm == "1": parent = MI(dataset, labels, className, attrVals)
  else: parent = cramersV(dataset, attributes, className)
  print(f"selected Parent:{parent}")

  plSubsets = {}
  for label in labels:
    subset = dataset[dataset[className] == label]
    pvDct = {}
    for pVal in attrVals[parent]:
      pvDct[pVal] = subset[subset[parent] == pVal]
    plSubsets[label] = pvDct

  PT = {attr:{label:{pVal:{} for pVal in attrVals[parent]} for label in labels} for attr in attributes if attr != parent}
  for attr in attributes:
    if attr == parent: continue
    for label in labels:
      branch = plSubsets[label]
      for pVal in attrVals[parent]:
        pls = branch[pVal]
        attrVC = pls[attr].value_counts().to_dict()
        for val in attrVals[attr]:
          count = attrVC.get(val, 0)
          value = (count + k)/(len(pls) + k*len(attrVals[attr]))
          PT[attr][label][pVal][val] = (count + k)/(len(pls) + k*len(attrVals[attr]))

  lCounts = dataset[className].value_counts().to_dict()
  lProbs = {label:lCounts.get(label, 0)/size for label in labels}
  numPV = len(attrVals[parent])
  plProbs = {label:{pVal:((len(plSubsets[label][pVal]) + k)/(lCounts.get(label, 0) + k*numPV)) for pVal in attrVals[parent]} for label in labels}
  unseenProbs = {attr:{label:(k / (lCounts.get(label, 0) + k*len(attrVals[attr]))) for label in labels} for attr in attributes}
  return parent, attrVals, PT, lProbs, plProbs, unseenProbs

#Mutual Information:
import math


def MI(dataset, labels, className, attrVals):
  superParent = ""
  maxMI = -1

  for parent in attrVals:
    miSum = 0
    for attr in attrVals:
      if parent == attr: continue
      miSum += calcMI(dataset, parent, attr, className, labels, attrVals)

    if(miSum >= maxMI):
      superParent = parent
      maxMI = miSum
  return superParent


def calcMI(dataset, parent, attr, className, labels, attrVals):
  size = len(dataset)
  lCounts = dataset[className].value_counts().to_dict()
  vlCounts = {val:dataset[dataset[attr] == val][className].value_counts().to_dict() for val in attrVals[attr]}

  mi = 0
  for pVal in attrVals[parent]:
    pSubset = dataset[dataset[parent] == pVal]
    plCounts = pSubset[className].value_counts().to_dict()

    for val in attrVals[attr]:
      pvSubset = pSubset[pSubset[attr] == val]
      pvlCounts = pvSubset[className].value_counts().to_dict()

      for label in labels:
        pvlc = pvlCounts.get(label, 0)
        if(pvlc == 0): continue

        lc = lCounts.get(label, 0)
        plc = plCounts.get(label, 0)
        vlc = vlCounts[val].get(label, 0)
        mi += (pvlc/size) * math.log((pvlc/lc) / ((plc/lc) * (vlc/lc)))

  return mi

#Correlation:
import numpy as np
from scipy.stats import chi2_contingency

def cramersV(dataset, attributes, className):
  parent = ""
  maxCramers = -1

  for attr in attributes:
    contingencyTable = pd.crosstab(dataset[attr], dataset[className])
    chi2, p, dof, expected = chi2_contingency(contingencyTable)
    n = contingencyTable.sum().sum()
    cramers = np.sqrt(chi2 / (n * (min(contingencyTable.shape) - 1)))

    if(cramers > maxCramers):
      parent = attr
      maxCramers = cramers
  return parent

#Artificial Immune System Weighted Naive Bayes:
import random
import math

def safe_log(x, eps=1e-10):
    return math.log(max(x, eps))


def evaluate_weight_vector_helper(instance, attributes, label, parent, labels, PT, lProbs, plProbs, unseenProbs, w):
    pVal = getattr(instance, parent)
    log_error = 0

    log_lProbs = {l: safe_log(lProbs[l]) for l in labels}
    log_plProbs = {l: {pv: safe_log(w[attributes.index(parent)] * plProbs[l].get(pv, unseenProbs[parent][l])) for pv in plProbs[l]} for l in labels}

    for l in labels:
        log_curProb = log_lProbs[l] + log_plProbs[l].get(pVal, safe_log(unseenProbs[parent][l]))

        for i, attr in enumerate(attributes):
            if attr == parent:
                continue
            val = getattr(instance, attr)

            try:
                log_curProb += safe_log(w[i] * PT[attr][l][pVal][val])
            except KeyError:
                log_curProb += safe_log(w[i] * unseenProbs[attr][l])

        if l == label:
            log_error += safe_log(1 - math.exp(log_curProb))
        else:
            log_error += log_curProb

    return -log_error


def evaluate_weight_vector(dataset, w, attributes, class_name, parent, labels, PT, lProbs, plProbs, unseenProbs):
    total_performance = 0
    for instance in dataset.itertuples():
        label = getattr(instance, class_name)
        error = evaluate_weight_vector_helper(instance, attributes, label, parent, labels, PT, lProbs, plProbs, unseenProbs, w)
        total_performance += -error
    return total_performance


def get_best_index(performance):
    best_idx = 0
    best_value = performance[0]
    for i in range(1, len(performance)):
        if performance[i] > best_value:
            best_value = performance[i]
            best_idx = i
    return best_idx


def get_worst_indices(performance, num_to_replace):
    return sorted(range(len(performance)), key=lambda i: performance[i])[:num_to_replace]



def AISWNB(dataset, parent, PT, lProbs, plProbs, unseenProbs, m=100, t=1e-4 , F=0.5, num_vectors=10):
  attributes = dataset.columns.tolist()
  class_name = attributes.pop()
  labels = dataset[class_name].unique()
  num_features = len(attributes)
  num_to_replace = num_vectors // 2

  weight_vectors = [[random.random()*0.5 + 0.5 for temp in range(num_features)] for temptemp in range(num_vectors)]

  for generation in range(m):
      performance = [evaluate_weight_vector(dataset, w, attributes, class_name, parent, labels, PT, lProbs, plProbs, unseenProbs) for w in weight_vectors]

      best_index = get_best_index(performance)
      best_vector = weight_vectors[best_index]

      worst_indices = list(get_worst_indices(performance, num_to_replace))
      for idx in worst_indices:
          weight_vectors[idx] = [min(1, max(0.0001, value + random.uniform(-0.05, 0.05))) for value in best_vector]


      for i in range(num_vectors):
          if i != best_index:
              noise = [(random.random() * 0.2 - 0.1) for _ in range(len(best_vector))]
              for j in range(len(best_vector)):
                  weight_vectors[i][j] = min(1, max(0.0001, weight_vectors[i][j] + F * noise[j]))

      improvement = max(performance) - min(performance)
      if improvement < t:
          print(f"Stopping early at generation {generation} due to low improvement.")
          break

  final_weight_vector = best_vector
  print("Final best performing weight vector:", final_weight_vector)
  return final_weight_vector

#Testing:
def test(dataset, parent, k, attrVals, PT, lProbs, plProbs, unseenProbs, weights):
  attributes = dataset.columns.tolist()
  className = attributes.pop()
  labels = dataset[className].unique()
  labelMap = {label:i for i, label in enumerate(labels)}
  confusionMatrix = [[0 for label in labels] for label in labels]

  numCorrect = 0
  for instance in dataset.itertuples():
    actual = getattr(instance, className)
    prediction = classifyInstance(instance, attributes, labels, parent, k, attrVals, PT, lProbs, plProbs, unseenProbs, weights)

    pIndex = labelMap[prediction]
    if prediction == actual:
      numCorrect += 1
      confusionMatrix[pIndex][pIndex] += 1
    else:
      aIndex = labelMap[actual]
      confusionMatrix[aIndex][pIndex] += 1

  print(numCorrect/len(dataset))
  return numCorrect/len(dataset), confusionMatrix


def classifyInstance(instance, attributes, labels, parent, k, attrVals, PT, lProbs, plProbs, unseenProbs, weights):
  pVal = getattr(instance, parent)
  prediction = ""
  maxProb = 0
  for label in labels:

    try:
      curProb = lProbs[label] * plProbs[label][pVal]
    except:
      curProb = lProbs[label] * unseenProbs[parent][label]

    for i, attr in enumerate(attributes):
      if attr == parent:
        curProb *= weights[i]
        continue
      val = getattr(instance, attr)

      try:
        curProb *= weights[i] * PT[attr][label][pVal][val]
      except:
        curProb *= weights[i] * unseenProbs[attr][label]

    if curProb > maxProb:
      prediction = label
      maxProb = curProb
  return prediction